{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4OGEu9nITbnO"
   },
   "source": [
    "# Install Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "uX687hj69g9g"
   },
   "outputs": [],
   "source": [
    "torchver = \"0.4.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1967,
     "status": "ok",
     "timestamp": 1527015382182,
     "user": {
      "displayName": "Sourabh Daptardar",
      "photoUrl": "//lh4.googleusercontent.com/-onn5Q0_MiKQ/AAAAAAAAAAI/AAAAAAAACDI/iOxkSEz16nA/s50-c-k-no/photo.jpg",
      "userId": "115812262388010820083"
     },
     "user_tz": -330
    },
    "id": "_gX52NUpzIYC",
    "outputId": "311649d4-8385-4c39-b984-1846818c2388"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: /opt/bin/nvidia-smi: not found\n",
      "Thu May 31 10:15:31 2018       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   34C    P0    37W / 300W |     98MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  Off  | 00000000:00:05.0 Off |                    0 |\n",
      "| N/A   36C    P0    35W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  Off  | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   37C    P0    38W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  Off  | 00000000:00:07.0 Off |                    0 |\n",
      "| N/A   36C    P0    35W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  Off  | 00000000:00:08.0 Off |                    0 |\n",
      "| N/A   34C    P0    38W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  Off  | 00000000:00:09.0 Off |                    0 |\n",
      "| N/A   35C    P0    37W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  Off  | 00000000:00:0A.0 Off |                    0 |\n",
      "| N/A   35C    P0    37W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  Off  | 00000000:00:0B.0 Off |                    0 |\n",
      "| N/A   35C    P0    37W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0      1655      G   /usr/lib/xorg/Xorg                            98MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!/opt/bin/nvidia-smi || /usr/bin/nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2037,
     "status": "ok",
     "timestamp": 1527015384289,
     "user": {
      "displayName": "Sourabh Daptardar",
      "photoUrl": "//lh4.googleusercontent.com/-onn5Q0_MiKQ/AAAAAAAAAAI/AAAAAAAACDI/iOxkSEz16nA/s50-c-k-no/photo.jpg",
      "userId": "115812262388010820083"
     },
     "user_tz": -330
    },
    "id": "Z0wFaqgbE4wI",
    "outputId": "feb79b53-fdc9-45eb-92c7-7dff6334c183"
   },
   "outputs": [],
   "source": [
    "# !ls /colabtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2121,
     "status": "ok",
     "timestamp": 1527015386438,
     "user": {
      "displayName": "Sourabh Daptardar",
      "photoUrl": "//lh4.googleusercontent.com/-onn5Q0_MiKQ/AAAAAAAAAAI/AAAAAAAACDI/iOxkSEz16nA/s50-c-k-no/photo.jpg",
      "userId": "115812262388010820083"
     },
     "user_tz": -330
    },
    "id": "G4WvjiCDzWPR",
    "outputId": "708f088a-9f78-4a08-9811-145b8874105b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.5 :: Anaconda, Inc.\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5041,
     "status": "ok",
     "timestamp": 1527015392777,
     "user": {
      "displayName": "Sourabh Daptardar",
      "photoUrl": "//lh4.googleusercontent.com/-onn5Q0_MiKQ/AAAAAAAAAAI/AAAAAAAACDI/iOxkSEz16nA/s50-c-k-no/photo.jpg",
      "userId": "115812262388010820083"
     },
     "user_tz": -330
    },
    "id": "z85P4eDZNSdu",
    "outputId": "26542569-3f96-4bc5-de5b-3d8c51c98f75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mSkipping pillow as it is not installed.\u001b[0m\n",
      "Collecting pillow-simd\n",
      "\u001b[31mtorchvision 0.2.1 requires pillow>=4.1.1, which is not installed.\u001b[0m\n",
      "Installing collected packages: pillow-simd\n",
      "  Found existing installation: Pillow-SIMD 5.1.1.post0\n",
      "    Uninstalling Pillow-SIMD-5.1.1.post0:\n",
      "      Successfully uninstalled Pillow-SIMD-5.1.1.post0\n",
      "Successfully installed pillow-simd-5.1.1.post0\n"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall -y pillow\n",
    "!CC=\"cc -mavx2\" pip3 install -U --force-reinstall pillow-simd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3414,
     "status": "ok",
     "timestamp": 1527015396225,
     "user": {
      "displayName": "Sourabh Daptardar",
      "photoUrl": "//lh4.googleusercontent.com/-onn5Q0_MiKQ/AAAAAAAAAAI/AAAAAAAACDI/iOxkSEz16nA/s50-c-k-no/photo.jpg",
      "userId": "115812262388010820083"
     },
     "user_tz": -330
    },
    "id": "7FiDFXCiT8wS",
    "outputId": "6a582c41-86d8-4fac-bdfd-b1008bba2099"
   },
   "outputs": [],
   "source": [
    "\n",
    "# !pip3 install ipdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6145,
     "status": "ok",
     "timestamp": 1527015402414,
     "user": {
      "displayName": "Sourabh Daptardar",
      "photoUrl": "//lh4.googleusercontent.com/-onn5Q0_MiKQ/AAAAAAAAAAI/AAAAAAAACDI/iOxkSEz16nA/s50-c-k-no/photo.jpg",
      "userId": "115812262388010820083"
     },
     "user_tz": -330
    },
    "id": "TAP3KzaO_3mr",
    "outputId": "d48fd867-01ff-479d-a37e-82c0ac00ce44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "PIL\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from os import path\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "\n",
    "pver = !python --version |& awk '{print $2 }' | awk -F. '{ print $1$2}'\n",
    "pyver = pver[0]\n",
    "print(pyver)\n",
    "\n",
    "# cver = !echo \"cu`nvcc --version | sed \"s/ /\\n/g\" | grep -i release -A 1 | tail -n 1 | tr -d [\\.,]`\"\n",
    "# cudaver = cver[0]\n",
    "cudaver = 'cu91'\n",
    "\n",
    "# accelerator =  cudaver if path.exists('/opt/bin/nvidia-smi') or path.exists('/usr/bin/nvidia-smi') else 'cpu'\n",
    "# print(accelerator)\n",
    "\n",
    "# torchurl = \"http://download.pytorch.org/whl/{0}/torch-{1}-cp{2}-cp{2}m-linux_x86_64.whl\".format(accelerator, torchver, pyver)\n",
    "# print(torchurl)\n",
    "\n",
    "# !pip3 install http://download.pytorch.org/whl/cu91/torch-0.4.0-cp36-cp36m-linux_x86_64.whl \n",
    "# !pip3 install torchvision\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "print(torchvision.get_image_backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3083,
     "status": "ok",
     "timestamp": 1527015405574,
     "user": {
      "displayName": "Sourabh Daptardar",
      "photoUrl": "//lh4.googleusercontent.com/-onn5Q0_MiKQ/AAAAAAAAAAI/AAAAAAAACDI/iOxkSEz16nA/s50-c-k-no/photo.jpg",
      "userId": "115812262388010820083"
     },
     "user_tz": -330
    },
    "id": "a4CFa1WLgoUX",
    "outputId": "cd632861-265b-4c66-bc57-fd7e2d6228ff"
   },
   "outputs": [],
   "source": [
    "#!pip3 install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GZt8MRT5RfK6"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ZptSyG9oSN1c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import io\n",
    "import time\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from argparse import Namespace\n",
    "from collections import OrderedDict\n",
    "from scipy.sparse import coo_matrix\n",
    "import socket\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "import hashlib\n",
    "import subprocess\n",
    "from copy import deepcopy, copy\n",
    "from pprint import pprint\n",
    "import torch.utils.data as data\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import namedtuple\n",
    "from PIL import Image \n",
    "from torchvision import get_image_backend\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.nn.init as weight_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IEEo0VYsZhvO"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1BQLwQTWcKU"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hostname_timestamp_id():\n",
    "    return socket.gethostname() + '_' + re.sub(r'\\W+', '', str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_fname():\n",
    "    return \"%s_%s_%s\" % (args.author, args.arch, get_hostname_timestamp_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "eM2a7qmqWh3Q"
   },
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "# base_dir = '/content/fashion'\n",
    "# args.perm_dir = '/data/datasets/kaggle_fashion'\n",
    "# args.base_dir = '/data/datasets/kaggle_fashion'\n",
    "args.perm_dir = '/mnt/disks/imaterialist_fashion'\n",
    "args.base_dir = '/mnt/ram-disk/imaterialist_fashion'\n",
    "args.data_dir = args.base_dir + os.sep + 'data'\n",
    "args.input_dir = args.data_dir + os.sep + 'input'\n",
    "args.output_dir = args.data_dir + os.sep + 'output'\n",
    "args.train_zip = args.input_dir + os.sep + 'train_data.zip'\n",
    "args.val_zip = args.input_dir + os.sep + 'validation_data.zip'\n",
    "args.train_dir = args.input_dir + os.sep + 'img_train'\n",
    "args.val_dir = args.input_dir + os.sep + 'img_val'\n",
    "args.test_dir = args.input_dir + os.sep + 'img_test'\n",
    "args.train_id = \"1rx1rL8RUAggN4hKlrYLtpdQagtUWmIbO\"\n",
    "args.val_id = \"1U19eWiBFJ6wGcFk47l6g9mmoWp1i4hPY\"\n",
    "# args.train_labels_id = \"1NOoWniR3ioqPKbVWoaWGy4HPDzZAAJX9\"\n",
    "args.train_labels_id = \"1X7TpWyxxtmCT5rw__7OKus_W4fh8xpKO\" # small dataset\n",
    "args.val_labels_id = \"1d9RuQTx5E8qFxraIu6B4rDTOC4sx2xXT\"\n",
    "args.test_labels_id = \"1VwzGCJfOL13pk1Wi-xPHQ6mVnofy9_Z4\"\n",
    "# args.train_labels_json = args.input_dir + os.sep + 'train.json'\n",
    "args.train_labels_json = args.input_dir + os.sep + 'train_small.json' \n",
    "# args.train_labels_json = args.input_dir + os.sep + 'train_tiny.json' \n",
    "args.val_labels_json = args.input_dir + os.sep + 'validation.json'\n",
    "args.test_labels_json = args.input_dir + os.sep + 'test.json'\n",
    "args.debug_weights = False\n",
    "args.test_overfit = False\n",
    "args.num_labels = 228\n",
    "args.batch_size = 16\n",
    "# args.batch_size = 64\n",
    "args.image_min_size = 256\n",
    "args.nw_input_size = 224\n",
    "args.num_workers = 4\n",
    "args.imagenet_mean = [0.485, 0.456, 0.406]\n",
    "args.imagenet_std = [0.229, 0.224, 0.225]\n",
    "args.pretrain_dset_mean = args.imagenet_mean\n",
    "args.pretrain_dset_std = args.imagenet_std\n",
    "args.world_size = 1\n",
    "args.dist_url = 'file://' + args.output_dir + os.sep + 'dfile'\n",
    "args.dist_backend = 'gloo'\n",
    "args.distributed = args.world_size > 1\n",
    "args.arch = 'resnet101'\n",
    "# args.arch = 'resnet152'\n",
    "args.fv_size = 2048\n",
    "args.pretrained = True\n",
    "args.resume = False\n",
    "args.start_epoch = 0\n",
    "args.small=1e-12                         # small value used for avoiding div by zero\n",
    "args.optimizer_learning_rate = 1e-4      # Adam optimizer initial learning rate\n",
    "args.scheduler_patience = 1              # Number of epochs with no improvement after which learning rate will be reduced\n",
    "args.scheduler_threshold = 1e-6          # learning rate scheduler threshold for measuring the new optimum, to only focus on significant changes\n",
    "args.scheduler_factor = 0.1        # learning rate scheduler factor by which the learning rate will be reduced. new_lr = lr * factor\n",
    "args.earlystopping_patience = 1          # early stopping patience is the number of epochs with no improvement after which training will be stopped\n",
    "args.earlystopping_min_delta = 1e-5      # minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement\n",
    "args.evaluate = False\n",
    "args.epochs = 2\n",
    "args.print_freq = args.batch_size\n",
    "args.ckpt_dir = args.output_dir + os.sep + 'ckpt'\n",
    "args.ckpt = args.ckpt_dir + os.sep + 'ckpt_%s.pth.tar' % (args.arch,)\n",
    "args.best = args.ckpt_dir + os.sep + 'best_%s.pth.tar' % (args.arch,)\n",
    "args.threshold = 0.5\n",
    "args.sub_dir = args.output_dir + os.sep + 'submissions'\n",
    "args.author = 'deccanlearners'\n",
    "args.output_id = get_output_fname()\n",
    "args.output_file = args.sub_dir + os.sep + 'output_%s.csv' %  args.output_id\n",
    "args.params_file = args.sub_dir + os.sep + 'params_%s.json' % args.output_id\n",
    "args.min_img_bytes = 4792"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 928
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1332,
     "status": "error",
     "timestamp": 1527015425722,
     "user": {
      "displayName": "Sourabh Daptardar",
      "photoUrl": "//lh4.googleusercontent.com/-onn5Q0_MiKQ/AAAAAAAAAAI/AAAAAAAACDI/iOxkSEz16nA/s50-c-k-no/photo.jpg",
      "userId": "115812262388010820083"
     },
     "user_tz": -330
    },
    "id": "90hY9m66UYPd",
    "outputId": "de44a4f5-386e-4243-b3bb-2da97134ce99"
   },
   "outputs": [],
   "source": [
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Kc-OLRcoHDBl"
   },
   "outputs": [],
   "source": [
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(d):\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def sha1_hash(fname, blocksize=4096):\n",
    "    \"\"\" compute sha1hash of a file \"\"\"\n",
    "    hash = ''\n",
    "    if not os.path.exists(fname):\n",
    "        errmsg = \"File %s does not exist\" % (fname)\n",
    "        print(errmsg)\n",
    "        return ''\n",
    "    try:\n",
    "        hasher = hashlib.sha1()\n",
    "        with open(fname, 'rb') as f:\n",
    "            buf = f.read(blocksize)\n",
    "            while len(buf) > 0:\n",
    "                hasher.update(buf)\n",
    "                buf = f.read(blocksize)\n",
    "        hash = hasher.hexdigest()\n",
    "    except:\n",
    "        print(\"Exception in hashing file\")\n",
    "        raise\n",
    "    return hash\n",
    "\n",
    "\n",
    "def rsync_and_verify(src, dst, verify=False, max_attempts=1):\n",
    "    \"\"\"Rsync src to dst and verify if copy is done\"\"\"\n",
    "\n",
    "    print('Rsync %s to %s on %s\\n' % (src,\n",
    "                                      dst,\n",
    "                                      socket.gethostname()))\n",
    "    sys.stdout.flush()\n",
    "    src_ = deepcopy(src)\n",
    "    dst_ = deepcopy(dst)\n",
    "    src_cred = ''\n",
    "    src_path = ''\n",
    "    dst_cred = ''\n",
    "    dst_path = ''\n",
    "    rsync_path = ''\n",
    "\n",
    "    if ':' in src:\n",
    "        src_cred, src_path = src.split(':')\n",
    "    else:\n",
    "        src_cred = ''\n",
    "        src_path = src\n",
    "\n",
    "    if ':' in dst:\n",
    "        dst_cred, dst_path = dst.split(':')\n",
    "    else:\n",
    "        dst_cred = ''\n",
    "        dst_path = dst\n",
    "\n",
    "    if src_cred == '':\n",
    "        mkdir_p(src_path)\n",
    "    else:\n",
    "        rsync_path = '--rsync-path=' + '\"' + 'mkdir -p' + ' ' + src_path + ' ' + '&&' + ' ' + 'rsync' + '\"'\n",
    "    \n",
    "    if dst_cred == '':\n",
    "        mkdir_p(dst_path)\n",
    "    else:\n",
    "        rsync_path = '--rsync-path=' + '\"' + 'mkdir -p' + ' ' + src_path + ' ' + '&&' + ' ' + 'rsync' + '\"'\n",
    "\n",
    "    if src_[-1] != os.sep:\n",
    "        src_ = src_ + os.sep\n",
    "    \n",
    "    if dst_[-1] != os.sep:\n",
    "        dst_ = dst_ + os.sep\n",
    "\n",
    "    for attempt in range(max_attempts):\n",
    "        print('attempt %d' % attempt)\n",
    "        try:\n",
    "            copycmd = 'rsync -av' + ' ' + rsync_path + ' ' + src_ + ' ' + dst_ \n",
    "            pprint(copycmd)\n",
    "            sys.stdout.flush()\n",
    "            output = subprocess.check_output(copycmd,\n",
    "                                             shell=True)\n",
    "            pprint(output)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            if verify:\n",
    "                # Verify if the copying is done correctly\n",
    "                if os.path.isdir(src):\n",
    "                    for fl in os.listdir(src):\n",
    "                        sfile = src + os.sep + fl\n",
    "                        dfile = dst + os.sep + fl\n",
    "                        shash = sha1_hash(sfile)\n",
    "                        dhash = sha1_hash(dfile)\n",
    "                        if shash != dhash:\n",
    "                            print('Hashes of files %s and %s do not match.' % (sfile, dfile))\n",
    "                            print('Error in copying. Quitting ...\\n')\n",
    "                            sys.stdout.flush()\n",
    "                            raise Exception('hash mismatch')\n",
    "                        print('.', end='')\n",
    "                        sys.stdout.flush()\n",
    "                else:\n",
    "                    shash = sha1_hash(src)\n",
    "                    dhash = sha1_hash(dst)\n",
    "                    if shash != dhash:\n",
    "                        print('Hashes of files %s and %s do not match.' % (src, dst))\n",
    "                        print('Error in copying. Quitting ...\\n')\n",
    "                        sys.stdout.flush()\n",
    "                        raise Exception('hash mismatch')\n",
    "                print('Hash check passed')\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            break    # break if successful\n",
    "        # except Exception, arg:\n",
    "        except:\n",
    "            # print('Error:', arg)\n",
    "            print('Error in rsync')\n",
    "            pass     # else retry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jB9hgpyUfbqG"
   },
   "outputs": [],
   "source": [
    "os.makedirs(args.base_dir, exist_ok=True)\n",
    "os.makedirs(args.data_dir, exist_ok=True)\n",
    "os.makedirs(args.input_dir, exist_ok=True)\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "os.makedirs(args.ckpt_dir, exist_ok=True)\n",
    "os.makedirs(args.sub_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsync_and_verify(args.perm_dir, args.base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dBC_aI1vRknn"
   },
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "g5eP3RxWV5L5"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "68MYkyHJWP0m"
   },
   "outputs": [],
   "source": [
    "# from googleapiclient.discovery import build\n",
    "# import io\n",
    "# from googleapiclient.http import MediaIoBaseDownload\n",
    "# import json\n",
    "\n",
    "# def md5_hash(fname, blocksize=4096):\n",
    "#     \"\"\" compute md5hash of a file \"\"\"\n",
    "#     import hashlib\n",
    "#     hash = ''\n",
    "#     if not os.path.exists(fname):\n",
    "#         errmsg = \"File %s does not exist\" % (fname)\n",
    "#         print(errmsg)\n",
    "#         return ''\n",
    "#     try:\n",
    "#         hasher = hashlib.md5()\n",
    "#         with open(fname, 'rb') as f:\n",
    "#             buf = f.read(blocksize)\n",
    "#             while len(buf) > 0:\n",
    "#                 hasher.update(buf)\n",
    "#                 buf = f.read(blocksize)\n",
    "#         hash = hasher.hexdigest()\n",
    "#     except:\n",
    "#         print(\"Exception in hashing file\")\n",
    "#         raise\n",
    "#     return hash\n",
    "\n",
    "# def _download(drive_service, file_id, loc):\n",
    "#   request = drive_service.files().get_media(fileId=file_id)\n",
    "#   fh = io.FileIO(loc, mode='wb')\n",
    "#   downloader = MediaIoBaseDownload(fh, request, chunksize=1024*1024)\n",
    "#   prev_progress = 0\n",
    "#   done = False\n",
    "#   with tqdm(total=100) as pbar:\n",
    "#     while done is False:\n",
    "#       status, done = downloader.next_chunk()\n",
    "#       if status:\n",
    "#         # print(\"Download %d%%.\" % int(status.progress() * 100))\n",
    "#         pbar.update(int(100 *(status.progress() - prev_progress)))\n",
    "#         prev_progress = status.progress()\n",
    "#   print(\"Download Complete!\")\n",
    "#   file_size = os.path.getsize(loc)\n",
    "#   print(\"Downloaded %d bytes\" % (file_size))\n",
    "\n",
    "# def download(file_id, loc):\n",
    "#   \"\"\"Downloads a file to local file system.\"\"\"  \n",
    "#   drive_service = build('drive', 'v3')\n",
    "  \n",
    "#   request_mdata = drive_service.files().list(fields=\"files(md5Checksum, originalFilename, id)\")\n",
    "#   rh = io.BytesIO()\n",
    "#   downloader_mdata = MediaIoBaseDownload(rh, request_mdata, chunksize=1024*1024)\n",
    "#   done = False\n",
    "#   while not done:\n",
    "#     _, done = downloader_mdata.next_chunk()\n",
    "#   mdata = json.loads(rh.getvalue())\n",
    "#   found = False\n",
    "#   md5drive = ''\n",
    "#   fname = ''\n",
    "#   for x in mdata['files']:\n",
    "#     if x['id'] == file_id:\n",
    "#       found = True\n",
    "#       md5drive = x['md5Checksum']\n",
    "#       fname = x['originalFilename']\n",
    "#       break\n",
    "#   if not found:\n",
    "#     print(\"{:s} : not found on gdrive\".format(file_id))\n",
    "#   else:\n",
    "#     if os.path.exists(loc):\n",
    "#       if md5drive == md5_hash(loc):\n",
    "#         print(\"{:s} : file already present on colab\".format(loc))\n",
    "#       else:\n",
    "#         print(\"{:s} [gdrive] and {:s} [colab] : md5 mismatch ... downloading\".format(fname, loc))\n",
    "#         _download(drive_service, file_id, loc)\n",
    "#     else:\n",
    "#       print(\"{:s} not present on colab ... downloading ...\".format(loc))\n",
    "#       _download(drive_service, file_id, loc)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8187,
     "status": "ok",
     "timestamp": 1527001525917,
     "user": {
      "displayName": "Sourabh Daptardar",
      "photoUrl": "//lh4.googleusercontent.com/-onn5Q0_MiKQ/AAAAAAAAAAI/AAAAAAAACDI/iOxkSEz16nA/s50-c-k-no/photo.jpg",
      "userId": "115812262388010820083"
     },
     "user_tz": -330
    },
    "id": "jOBHlpMKaE-F",
    "outputId": "71d5fe19-81cd-4428-90c7-8e0f720772b4"
   },
   "outputs": [],
   "source": [
    "# download(args.train_id, args.train_zip)\n",
    "# download(args.val_id, args.val_zip)\n",
    "# download(args.train_labels_id, args.train_labels_json)\n",
    "# download(args.val_labels_id, args.val_labels_json)\n",
    "# download(args.test_labels_id, args.test_labels_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mA1kgVVEdSWI"
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.unpack_archive(args.train_zip, args.input_dir)\n",
    "# shutil.unpack_archive(args.val_zip, args.input_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2944,
     "status": "ok",
     "timestamp": 1527001543581,
     "user": {
      "displayName": "Sourabh Daptardar",
      "photoUrl": "//lh4.googleusercontent.com/-onn5Q0_MiKQ/AAAAAAAAAAI/AAAAAAAACDI/iOxkSEz16nA/s50-c-k-no/photo.jpg",
      "userId": "115812262388010820083"
     },
     "user_tz": -330
    },
    "id": "X4Eugqw2fJRQ",
    "outputId": "76b1998f-7f77-4cfb-ea39-39fa50283aca"
   },
   "outputs": [],
   "source": [
    "# !ls -ltr /content/fashion/data/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2245,
     "status": "ok",
     "timestamp": 1527001545898,
     "user": {
      "displayName": "Sourabh Daptardar",
      "photoUrl": "//lh4.googleusercontent.com/-onn5Q0_MiKQ/AAAAAAAAAAI/AAAAAAAACDI/iOxkSEz16nA/s50-c-k-no/photo.jpg",
      "userId": "115812262388010820083"
     },
     "user_tz": -330
    },
    "id": "_XnHDCmclB9B",
    "outputId": "7a665530-a452-41d9-ba5f-578562e6da35"
   },
   "outputs": [],
   "source": [
    "# !ls -ltr /content/fashion/data/input/train_data | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2216,
     "status": "ok",
     "timestamp": 1527001548219,
     "user": {
      "displayName": "Sourabh Daptardar",
      "photoUrl": "//lh4.googleusercontent.com/-onn5Q0_MiKQ/AAAAAAAAAAI/AAAAAAAACDI/iOxkSEz16nA/s50-c-k-no/photo.jpg",
      "userId": "115812262388010820083"
     },
     "user_tz": -330
    },
    "id": "4bmRgA9ilISL",
    "outputId": "db0b777e-bbc6-4326-c467-64779ca51b3b"
   },
   "outputs": [],
   "source": [
    "# !ls -ltr /content/fashion/data/input/validation_data | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c6LY5l-SRtWw"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VMcIw45smeZE"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "\n",
    "def fetch_labels(annotations, num_labels):\n",
    "  labels = OrderedDict()\n",
    "  for x in annotations:\n",
    "    arr = np.zeros((num_labels,), dtype=np.float32)\n",
    "    for y in map(int, x['labelId']):\n",
    "      arr[y-1] = 1.0\n",
    "    labels[int(x['imageId'])] = copy(arr)\n",
    "  return labels\n",
    "\n",
    "def json_to_dict(fpath):\n",
    "  import json\n",
    "  with open(fpath) as f: \n",
    "    D = json.load(f)\n",
    "  return D\n",
    "\n",
    "def get_labelinfo(annotations):\n",
    "  from collections import namedtuple\n",
    "  labelinfo = namedtuple('labelinfo', \"set min max count\")\n",
    "  labelinfo.set = set()\n",
    "  for x in annotations:\n",
    "    labelinfo.set.update(map(int, x['labelId']))\n",
    "  labelinfo.min = min(labelinfo.set)\n",
    "  labelinfo.max = max(labelinfo.set)\n",
    "  labelinfo.count = len(labelinfo.set)\n",
    "  return labelinfo\n",
    "\n",
    "def has_file_allowed_extension(filename, extensions):\n",
    "    \"\"\"Checks if a file is an allowed extension.\n",
    "    Args:\n",
    "        filename (string): path to a file\n",
    "    Returns:\n",
    "        bool: True if the filename ends with a known image extension\n",
    "    \"\"\"\n",
    "    filename_lower = filename.lower()\n",
    "    return any(filename_lower.endswith(ext) for ext in extensions)\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    from PIL import Image \n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)\n",
    "\n",
    "      \n",
    "class FashionDataset(data.Dataset):\n",
    "  \"\"\"Fashion dataset CVPR challenge.\n",
    "     Adapted from torchvision ImageFolder.\n",
    "     Similar to ImageFolder with the following differences:\n",
    "     1. Multilabel\n",
    "     2. Directory structure where all images are directly in the root folder\n",
    "     3. Labels are read from json file\n",
    "   \n",
    "  Args:\n",
    "        root (string): Root directory path.\n",
    "        loader (callable): A function to load a sample given its path.\n",
    "        extensions (list[string]): A list of allowed extensions.\n",
    "        transform (callable, optional): A function/transform that takes in\n",
    "            a sample and returns a transformed version.\n",
    "            E.g, ``transforms.RandomCrop`` for images.\n",
    "        target_transform (callable, optional): A function/transform that takes\n",
    "            in the target and transforms it.\n",
    "    \n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, root, metadata_file, num_labels=228, transform=None, target_transform=None,\n",
    "               loader=default_loader, test=False, min_img_bytes=4792):\n",
    "    extensions = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n",
    "    self.test = test\n",
    "    self.num_labels = num_labels\n",
    "    self.images_ = OrderedDict()\n",
    "    self.images = OrderedDict()\n",
    "    self.metadata_file = metadata_file\n",
    "    self.metadata = json_to_dict(self.metadata_file)\n",
    "    self.transform = transform\n",
    "    self.root = root\n",
    "    self.target_transform = target_transform\n",
    "    self.loader = loader\n",
    "    self.corrupt = 0\n",
    "    self.corrupt_ids = set()\n",
    "    self.labels = OrderedDict()\n",
    "    self.labels_ = OrderedDict()\n",
    "    \n",
    "    # Fetch labels\n",
    "    if not self.test:\n",
    "        self.labels_ = fetch_labels(self.metadata['annotations'], self.num_labels)\n",
    "\n",
    "    # Create Image list\n",
    "    for x in self.metadata['images']:\n",
    "      self.images_[int(x['imageId'])] = '%s%s%d.jpg' % (root, os.sep, int(x['imageId']))\n",
    "        \n",
    "    # Remove corrupt image files\n",
    "    ids = self.images_.keys()\n",
    "    for i in tqdm(ids):\n",
    "        ## Correct but slow\n",
    "#         try:\n",
    "#             img = self.loader(self.images_[i])\n",
    "#             img.close()\n",
    "#         except:\n",
    "#             self.corrupt += 1\n",
    "#             self.corrupt_ids.add(i)\n",
    "        ## Optimistic \n",
    "        if os.path.getsize(self.images_[i]) < min_img_bytes:\n",
    "            self.corrupt += 1\n",
    "            self.corrupt_ids.add(i)\n",
    "\n",
    "    for i in ids:\n",
    "        if i not in self.corrupt_ids:\n",
    "            self.images[i] = copy(self.images_[i])\n",
    "            if not self.test:\n",
    "                self.labels[i] = copy(self.labels_[i])\n",
    "    self.image_ids = list(self.images.keys())\n",
    "    \n",
    "    if not self.test:\n",
    "        self.labelinfo = get_labelinfo(self.metadata['annotations'])\n",
    "    \n",
    "  def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (sample, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        if not self.test:\n",
    "            path, target = self.images[self.image_ids[index]], self.labels[self.image_ids[index]]\n",
    "        else:\n",
    "            path = self.images[self.image_ids[index]]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if not self.test:\n",
    "            if self.target_transform is not None:\n",
    "                target = self.target_transform(target)\n",
    "        \n",
    "        if self.test:\n",
    "            return sample\n",
    "        else:\n",
    "            return sample, target\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.images)\n",
    "  \n",
    "  def __repr__(self):\n",
    "    fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "    fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "    fmt_str += '    Number of corrupt datapoints discarded: {}\\n'.format(self.corrupt)\n",
    "    if not self.test:\n",
    "        fmt_str += '    Number of labels: {}\\n'.format(self.labelinfo.count)\n",
    "    fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "    fmt_str += '    Metadata file: {}\\n'.format(self.metadata_file)\n",
    "    tmp = '    Transforms (if any): '\n",
    "    fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "    if not self.test:\n",
    "        tmp = '    Target Transforms (if any): '\n",
    "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "    tmp = '    Loader: '\n",
    "    fmt_str += '\\n{0}{1}'.format(tmp, self.loader.__name__)\n",
    "    return fmt_str\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BuB8IhRXrZzK"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "def create_transforms(args):\n",
    "    if args.test_overfit:\n",
    "        train_tform = transforms.Compose([transforms.Resize(args.image_min_size),\n",
    "                                          transforms.CenterCrop(args.nw_input_size),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize(mean=args.pretrain_dset_mean,\n",
    "                                                                std=args.pretrain_dset_std)\n",
    "                                         ])\n",
    "    else:\n",
    "        train_tform = transforms.Compose([transforms.RandomResizedCrop(args.nw_input_size),\n",
    "                                          transforms.RandomHorizontalFlip(),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize(mean=args.pretrain_dset_mean,\n",
    "                                                                std=args.pretrain_dset_std)\n",
    "                                         ])\n",
    "\n",
    "    val_tform = transforms.Compose([transforms.Resize(args.image_min_size),\n",
    "                                    transforms.CenterCrop(args.nw_input_size),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=args.pretrain_dset_mean,\n",
    "                                                         std=args.pretrain_dset_std)\n",
    "                                   ])\n",
    "    return (train_tform, val_tform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tform, val_tform = create_transforms(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 981,
     "status": "ok",
     "timestamp": 1527001551536,
     "user": {
      "displayName": "Sourabh Daptardar",
      "photoUrl": "//lh4.googleusercontent.com/-onn5Q0_MiKQ/AAAAAAAAAAI/AAAAAAAACDI/iOxkSEz16nA/s50-c-k-no/photo.jpg",
      "userId": "115812262388010820083"
     },
     "user_tz": -330
    },
    "id": "b7JjnbO4a1bU",
    "outputId": "d20390c8-2b46-4399-bd70-7486521b4976",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dset = FashionDataset(args.train_dir, args.train_labels_json, args.num_labels, transform=train_tform, min_img_bytes=args.min_img_bytes)\n",
    "val_dset = FashionDataset(args.val_dir, args.val_labels_json, args.num_labels, transform=val_tform, min_img_bytes=args.min_img_bytes)\n",
    "test_dset = FashionDataset(args.test_dir, args.test_labels_json, args.num_labels, transform=val_tform, test=True, min_img_bytes=args.min_img_bytes) # same transform as validation\n",
    "\n",
    "\n",
    "print(train_dset)\n",
    "print(val_dset)\n",
    "print(test_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-EoLW0no-em7"
   },
   "outputs": [],
   "source": [
    "def tensor_to_numpy(t, avg, std):\n",
    "  return (255.0 * (np.transpose(np.asarray(t), (1, 2, 0)) * std + avg)).astype(np.uint8)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1703,
     "status": "ok",
     "timestamp": 1527001554370,
     "user": {
      "displayName": "Sourabh Daptardar",
      "photoUrl": "//lh4.googleusercontent.com/-onn5Q0_MiKQ/AAAAAAAAAAI/AAAAAAAACDI/iOxkSEz16nA/s50-c-k-no/photo.jpg",
      "userId": "115812262388010820083"
     },
     "user_tz": -330
    },
    "id": "L8qbOaD8HvYi",
    "outputId": "4e64893b-2094-4f5f-c9cd-6469eb2eaa8e"
   },
   "outputs": [],
   "source": [
    "rnd1 = np.random.randint(len(train_dset))\n",
    "im1, lbl1 = train_dset[rnd1]\n",
    "imshow(tensor_to_numpy(im1, args.pretrain_dset_mean, args.pretrain_dset_std))\n",
    "print(lbl1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1502,
     "status": "ok",
     "timestamp": 1527001555965,
     "user": {
      "displayName": "Sourabh Daptardar",
      "photoUrl": "//lh4.googleusercontent.com/-onn5Q0_MiKQ/AAAAAAAAAAI/AAAAAAAACDI/iOxkSEz16nA/s50-c-k-no/photo.jpg",
      "userId": "115812262388010820083"
     },
     "user_tz": -330
    },
    "id": "3h5_M6G0QBYZ",
    "outputId": "01e18eb7-caf7-4bcb-82b1-b296b9784185"
   },
   "outputs": [],
   "source": [
    "rnd2 = np.random.randint(len(val_dset))\n",
    "im2, lbl2 = val_dset[rnd2]\n",
    "imshow(tensor_to_numpy(im2, args.pretrain_dset_mean, args.pretrain_dset_std))\n",
    "print(lbl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd3 = np.random.randint(len(test_dset))\n",
    "im3 = test_dset[rnd3]\n",
    "imshow(tensor_to_numpy(im3, args.pretrain_dset_mean, args.pretrain_dset_std))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iU_VDQm2Rtro"
   },
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4KcwWuuHoxoo"
   },
   "outputs": [],
   "source": [
    "if args.distributed:\n",
    "    dist.init_process_group(backend=args.dist_backend,\n",
    "                            init_method=args.dist_url,\n",
    "                            world_size=args.world_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EjMlN6vqHtsE"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler \n",
    "\n",
    "\n",
    "if args.distributed:\n",
    "  train_sampler = DistributedSampler(train_dset)\n",
    "else:\n",
    "  train_sampler = None\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           shuffle=(train_sampler is None),\n",
    "                                           num_workers=args.num_workers,\n",
    "                                           pin_memory=True,\n",
    "                                           sampler=train_sampler\n",
    "                                          )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dset,\n",
    "                                         batch_size=args.batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=args.num_workers,\n",
    "                                         pin_memory=True\n",
    "                                        )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dset,\n",
    "                                         batch_size=args.batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=args.num_workers,\n",
    "                                         pin_memory=True\n",
    "                                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "uzNG4-7x6Ovt"
   },
   "outputs": [],
   "source": [
    "# train_images, train_labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1527001561251,
     "user": {
      "displayName": "Sourabh Daptardar",
      "photoUrl": "//lh4.googleusercontent.com/-onn5Q0_MiKQ/AAAAAAAAAAI/AAAAAAAACDI/iOxkSEz16nA/s50-c-k-no/photo.jpg",
      "userId": "115812262388010820083"
     },
     "user_tz": -330
    },
    "id": "CDOOVYOHbcbl",
    "outputId": "46f258e9-ee85-4435-8df8-57b8912d5ced"
   },
   "outputs": [],
   "source": [
    "# rnd11 = np.random.randint(args.batch_size)\n",
    "# print(train_images[rnd11,:,:,:])\n",
    "# print(train_labels[rnd11, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IZ7R4Mgb7F3b"
   },
   "outputs": [],
   "source": [
    "# val_images, val_labels = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 918,
     "status": "ok",
     "timestamp": 1527001564208,
     "user": {
      "displayName": "Sourabh Daptardar",
      "photoUrl": "//lh4.googleusercontent.com/-onn5Q0_MiKQ/AAAAAAAAAAI/AAAAAAAACDI/iOxkSEz16nA/s50-c-k-no/photo.jpg",
      "userId": "115812262388010820083"
     },
     "user_tz": -330
    },
    "id": "CIOHOwAqaRkX",
    "outputId": "7b674419-1de5-4765-c7e4-c2db097f3170"
   },
   "outputs": [],
   "source": [
    "# rnd21 = np.random.randint(args.batch_size)\n",
    "# print(val_images[rnd21,:,:,:])\n",
    "# print(val_labels[rnd21, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cQAQrfMJRtv3"
   },
   "source": [
    "\n",
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jNxCNVyu98GF"
   },
   "outputs": [],
   "source": [
    "import torch.nn.init as weight_init\n",
    "\n",
    "\n",
    "class FCWithLogSigmoid(nn.Module):\n",
    "  \n",
    "  def __init__(self, num_inputs, num_outputs):\n",
    "    super(FCWithLogSigmoid, self).__init__()\n",
    "    self.linear = nn.Linear(num_inputs, num_outputs)\n",
    "    self.logsigmoid = nn.LogSigmoid()\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.logsigmoid(self.linear(x))\n",
    "\n",
    "\n",
    "def create_model(arch, num_labels=228, fv_size=2048, pretrained=True, resume=False, distributed=False):\n",
    "  if pretrained:\n",
    "      print(\"=> using pre-trained model '{}'\".format(arch))\n",
    "      model = models.__dict__[arch](pretrained=True)\n",
    "  else:\n",
    "      print(\"=> creating model '{}'\".format(arch))\n",
    "      model = models.__dict__[arch]()\n",
    "  model.fc = FCWithLogSigmoid(fv_size, num_labels)\n",
    "  if not distributed:\n",
    "      if arch.startswith('alexnet') or arch.startswith('vgg'):\n",
    "          model.features = torch.nn.DataParallel(model.features)\n",
    "          model.cuda()\n",
    "      else:\n",
    "          model = torch.nn.DataParallel(model).cuda()\n",
    "  else:\n",
    "      model.cuda()\n",
    "      model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9eMpjddlO6BC"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "  \"\"\"source: https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/9\"\"\"\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1945,
     "status": "ok",
     "timestamp": 1527001568263,
     "user": {
      "displayName": "Sourabh Daptardar",
      "photoUrl": "//lh4.googleusercontent.com/-onn5Q0_MiKQ/AAAAAAAAAAI/AAAAAAAACDI/iOxkSEz16nA/s50-c-k-no/photo.jpg",
      "userId": "115812262388010820083"
     },
     "user_tz": -330
    },
    "id": "CDhvVsKk_cNI",
    "outputId": "e3b9d484-4d9e-4fbd-c7d2-799d0d838ab9"
   },
   "outputs": [],
   "source": [
    "model = create_model(args.arch,\n",
    "                     num_labels=args.num_labels,\n",
    "                     fv_size=args.fv_size,\n",
    "                     pretrained=args.pretrained,\n",
    "                     resume=args.resume,\n",
    "                     distributed=args.distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 990,
     "status": "ok",
     "timestamp": 1527001569282,
     "user": {
      "displayName": "Sourabh Daptardar",
      "photoUrl": "//lh4.googleusercontent.com/-onn5Q0_MiKQ/AAAAAAAAAAI/AAAAAAAACDI/iOxkSEz16nA/s50-c-k-no/photo.jpg",
      "userId": "115812262388010820083"
     },
     "user_tz": -330
    },
    "id": "yG1C75oXPLx8",
    "outputId": "c1d7369f-4563-4fbf-d155-62227edccd93"
   },
   "outputs": [],
   "source": [
    "print(\"Neural Network has \", count_parameters(model), \" trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightUpdateTracker:\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        with torch.no_grad():\n",
    "            self.num_param_tensors = len(list(model.parameters()))\n",
    "            self.prev_pnorms = torch.zeros(self.num_param_tensors) \n",
    "            self.curr_pnorms = self.parameter_norms(model) \n",
    "\n",
    "    def parameter_norms(self, model):\n",
    "        with torch.no_grad():\n",
    "            pnorms = torch.zeros(self.num_param_tensors)\n",
    "            for i, x in enumerate(list(model.parameters())):\n",
    "                pnorms[i] = x.norm().item()\n",
    "            return pnorms\n",
    "        \n",
    "    def track(self, model):\n",
    "        with torch.no_grad():\n",
    "            self.prev_pnorms = self.curr_pnorms.clone()\n",
    "            self.curr_pnorms = self.parameter_norms(model)\n",
    "            self.delta = (self.curr_pnorms - self.prev_pnorms) / self.prev_pnorms\n",
    "\n",
    "            \n",
    "    def __repr__(self):\n",
    "        with torch.no_grad():\n",
    "            return self.delta.__repr__()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VIILcEp9Rtz-"
   },
   "source": [
    "# Loss Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PICCxotzRt4z"
   },
   "source": [
    "# Update Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zaX2mCHTDgSi"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       amsgrad=True,\n",
    "                       lr=args.optimizer_learning_rate,\n",
    "                       betas=(0.9, 0.999),\n",
    "                       eps=1e-8,\n",
    "                       weight_decay=0.0\n",
    "                      )\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                           mode='max',   # F1 measure\n",
    "                                           patience=args.scheduler_patience,\n",
    "                                           threshold=args.scheduler_threshold,\n",
    "                                           factor=args.scheduler_factor,\n",
    "                                           verbose=1\n",
    "                                          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tCm_msJ0RuIu"
   },
   "source": [
    "# Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, scheduler, args, resume=True, ckpt=None):\n",
    "    \"\"\"optionally resume from a checkpoint.\"\"\"\n",
    "    best_f1 = 0\n",
    "    if args.resume:\n",
    "        if os.path.isfile(ckpt):\n",
    "            print(\"=> loading checkpoint '{}'\".format(ckpt))\n",
    "            checkpoint = torch.load(ckpt)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_f1 = checkpoint['best_f1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "          #  scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(ckpt))\n",
    "            best_f1 = 0\n",
    "    return (model, optimizer, scheduler, args, best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar', best_model_filename='model_best.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, best_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1MicroAverageMeter(object):\n",
    "    \"\"\"Computes and stores F1 store\"\"\"\n",
    "    def __init__(self, threshold=0.5, small=1e-12):\n",
    "        self.threshold = threshold\n",
    "        self.small = small\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.TP = 0.0\n",
    "        self.FP = 0.0\n",
    "        self.FN = 0.0\n",
    "        self.TN = 0.0\n",
    "        self.precision = 0.0\n",
    "        self.recall = 0.0\n",
    "        self.f1 = 0.0\n",
    "\n",
    "    def update(self, labels, pred):\n",
    "        tp, fp, fn, tn = self.confusion_matrix_(labels, pred)\n",
    "        self.TP += tp\n",
    "        self.FP += fp\n",
    "        self.FN += fn\n",
    "        self.TN += tn\n",
    "        self.precision = self.TP / (self.small + self.TP + self.FP)\n",
    "        self.recall = self.TP / (self.small + self.TP + self.FN)\n",
    "        self.f1 = (2.0 * self.precision * self.recall) / (self.small + self.precision + self.recall)\n",
    "        \n",
    "    def confusion_matrix_(self, labels, pred):\n",
    "        with torch.no_grad():\n",
    "            real = labels\n",
    "            fake = 1.0 - real\n",
    "            pos = pred.ge(self.threshold)\n",
    "            pos = pos.float()\n",
    "            neg = 1.0 - pos\n",
    "            tp = torch.sum(real * pos).item()\n",
    "            fp = torch.sum(fake * pos).item()\n",
    "            fn = torch.sum(real * neg).item()\n",
    "            tn = torch.sum(fake * neg).item()\n",
    "            return (tp, fp, fn, tn)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, scheduler, epoch, measure, args):\n",
    "    if not args.test_overfit:\n",
    "        scheduler.step(measure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    cmpoint5 = F1MicroAverageMeter(threshold=0.5)\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda(non_blocking=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure F1 and record loss\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        cmpoint5.update(target, torch.exp(output))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        \n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Precision {cmpoint5.precision:.3f}\\t'\n",
    "                  'Recall {cmpoint5.recall:.3f}\\t'\n",
    "                  'F1 {cmpoint5.f1:.3f}'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, cmpoint5=cmpoint5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    cmpoint5 = F1MicroAverageMeter(threshold=0.5)\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.cuda(non_blocking=True)\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure F1 and record loss\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            cmpoint5.update(target, torch.exp(output))\n",
    "            \n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % args.print_freq == 0:\n",
    "                 print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Precision {cmpoint5.precision:.3f}\\t'\n",
    "                      'Recall {cmpoint5.recall:.3f}\\t'\n",
    "                      'F1 {cmpoint5.f1:.3f}'.format(\n",
    "                       i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                       cmpoint5=cmpoint5))\n",
    "\n",
    "        print(' * Precision {cmpoint5.precision:.3f} Recall {cmpoint5.recall:.3f} F1 {cmpoint5.f1:.3f}'\n",
    "              .format(cmpoint5=cmpoint5))\n",
    "\n",
    "    return cmpoint5.f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ofname, pfname, args, test_dset, test_loader, best_model_ckpt, model, threshold=0.5, epoch=0):\n",
    "    \n",
    "#     checkpoint = torch.load(best_model_ckpt)\n",
    "#     model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    res = OrderedDict()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, input in enumerate(test_loader):\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            spout = coo_matrix(torch.exp(output).ge(threshold).int().cpu().numpy())\n",
    "            for p in zip(spout.row, spout.col):\n",
    "                imid = test_dset.image_ids[i* args.batch_size+p[0]]\n",
    "                if imid not in res.keys():\n",
    "                    res[imid] = [p[1]+1]\n",
    "                else:\n",
    "                    res[imid].append(p[1]+1)\n",
    "            \n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % args.print_freq == 0:\n",
    "                 print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'.format(\n",
    "                       i, len(test_loader), batch_time=batch_time))\n",
    "    \n",
    "    ofname_ = \"%s%s%03d_%s\" % (os.path.dirname(ofname), os.sep, epoch, os.path.basename(ofname))\n",
    "    with open(ofname_, \"w\") as ofd:\n",
    "        ofd.write(\"image_id,label_id\\n\")\n",
    "        for k, v in res.items():\n",
    "            ofd.write(\"%d,%s\\n\" % (k, \" \".join(map(str,v))))\n",
    "            \n",
    "    pfname_ = \"%s%s%03d_%s\" % (os.path.dirname(pfname), os.sep, epoch, os.path.basename(pfname))\n",
    "    with open(pfname_, \"w\") as pfd:\n",
    "        json.dump(vars(args), pfd, sort_keys=True, indent=4)\n",
    "            \n",
    "    print(\"Output written to %s\\n\" % ofname_)\n",
    "    print(\"Program parameters written to %s\\n\" % pfname_)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_loader, val_loader, test_loader, test_dset, args, optimizer, scheduler, model, criterion, threshold=0.5):\n",
    "    if args.evaluate:\n",
    "        validate(val_loader, model, criterion)\n",
    "    else:\n",
    "        model, optimizer, scheduler, args, best_f1 = load_checkpoint(model, optimizer, scheduler, args, resume=args.resume, ckpt=args.ckpt)\n",
    "        wut = None\n",
    "        if args.debug_weights:\n",
    "            wut = WeightUpdateTracker(model)\n",
    "        for epoch in range(args.start_epoch, args.epochs):\n",
    "            if args.distributed:\n",
    "                train_sampler.set_epoch(epoch)\n",
    "    #         adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "            # train for one epoch\n",
    "            train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "            if args.debug_weights:\n",
    "                # debug: track weight updates\n",
    "                wut.track(model)\n",
    "                print(wut)\n",
    "\n",
    "            # evaluate on validation set\n",
    "            f1 = validate(val_loader, model, criterion)\n",
    "\n",
    "            # remember best f1 and save checkpoint\n",
    "            is_best = f1 > best_f1\n",
    "            best_f1 = max(f1, best_f1)\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'arch': args.arch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_f1': best_f1,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "               # 'scheduler' : scheduler.state_dict(),\n",
    "            }, is_best, filename=args.ckpt, best_model_filename=args.best)\n",
    "\n",
    "            if is_best:\n",
    "                print(\"BEST: \", epoch)\n",
    "                sys.stdout.flush()\n",
    "            adjust_learning_rate(optimizer, scheduler, epoch, f1, args)\n",
    "            test(args.output_file, args.params_file, args, test_dset, test_loader, args.best, model, threshold=args.threshold, epoch=epoch)            \n",
    "            rsync_and_verify(args.base_dir, args.perm_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(train_loader, val_loader, test_loader, test_dset, args, optimizer, scheduler, model, criterion, threshold=args.threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move inference inside training loop for results from partially trained model\n",
    "#test(args.output_file, args.params_file, args, test_dset, test_loader, args.best, model, threshold=args.threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gevaiXFORuTH"
   },
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XlL1brDNRucP"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PT9Shf_MRhui"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "TrainLoop.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
